---
title: "Take home assignment 2 - Glenn"
execute: 
  warning: false
  #eval: false
  
date: "r Sys.Date()`"
---

# 1.0 The Task

The specific tasks of this take-home exercise are as follows:

-Using appropriate function of sf and tidyverse, preparing the following geospatial data layer: 1. a study area layer in sf polygon features. It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan. 2. a dengue fever layer within the study area in sf point features. The dengue fever cases should be confined to epidemiology week 31-50, 2023. 3. a derived dengue fever layer in spacetime s3 class of sfdep. It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.

-Using the extracted data, perform global spatial autocorrelation analysis by using sfdep methods. -Using the extracted data, perform local spatial autocorrelation analysis by using sfdep methods. -Using the extracted data, perform emerging hotspot analysis by using sfdep methods. -Describe the spatial patterns revealed by the analysis above.

# 2.0 Importing the packages

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse,spdep)
```

# 3.0 Data Wrangling

## 3.1 Importing the data (Tainan)

```{r}
tainan <- st_read(dsn = "data/geospatial", 
                 layer = "TAINAN_VILLAGE") %>% st_transform(crs = 3824)
```

```{r}
plot(st_geometry(tainan))
```

We plot the map of tainan and see that it is not the same as shown in class. This is because it includes all the regions and tainan. We have to filter the dataset accordingly in later code so that we get the correct area.

## 3.2 Importing the data (Dengue Daily)

```{r}
dengue <- read_csv("data/aspatial/Dengue_Daily.csv")
```

## 3.3 Making the list of towns required - Found in assignment brief

```{r}
town_ids <- c("D01","D02","D04","D06","D07","D08","D32","D39")
```

town_ids is a list of towns, as mentioned in the assignment brief on the website

## 3.3.1 Filtering tainan villages based on the above list

```{r}
tainan_filtered_villages <- tainan %>%
  filter(TOWNID %in% town_ids)
```

We then filter our dataset to only include those towns that are provided in the list

```{r}
tainan_filtered_villages
```

We then plot out the map to see if the area is the same as shown in class

```{r}
plot(st_geometry(tainan_filtered_villages))
```

After filtering the dataset, we can see that the plot is indeed the same as the one shown in class.

## 3.4 Filtering dengue dataset to only fall between epidemiology week 31-50, 2023.

### 3.4.1 Changing first column to transmit date - easier

```{r}
colnames(dengue)[1] <- "transmit_date"
```

### 3.4.2 Changing column type to date

```{r}
dengue$transmit_date <- as.Date(dengue$transmit_date)
```

### 3.4.3 Filtering for week 31 - 50

Epidemoiology weeks source- https://www.hpsc.ie/notifiablediseases/resources/epidemiologicalweeks/

```{r}
dengue_filtered <- dengue %>% filter(between(transmit_date, as.Date('2023-07-30'), as.Date('2023-12-16')))

dengue_filtered$week_number <- week(dengue_filtered$transmit_date) - week('2023-07-30') + 1

head(dengue_filtered)
```

### 3.4.4 Only keeping date transmit , lat and long and week_number

```{r}
dengue_filtered <- select(dengue_filtered,1,10,11,27)
colnames(dengue_filtered)[2] <- "lng"
colnames(dengue_filtered)[3] <- "lat"
```

```{r}
st_crs(tainan_filtered_villages)
```

We check the CRS of our dataset to see if it is correct. Correct CRS should be TWD97, not WSG84

## 3.5 Joining the data of filtered village and dengue

```{r}
dengue_filtered
```

### 3.5.1 Changing class from character to numeric

```{r}
dengue_filtered$lng <- as.numeric(dengue_filtered$lng)
dengue_filtered$lat <- as.numeric(dengue_filtered$lat)
```

### 3.5.2 Removing NA Values

```{r}
dengue_filtered_complete <- na.omit(dengue_filtered)
```

NA Values are introduced and we have to remove them

### 3.5.3 Transforming from decimal point to 3824

```{r}

sf_object <- st_as_sf(dengue_filtered_complete, coords = c("lng", "lat"), crs = 4326) %>% st_transform(crs = 3824)
```

Looking at the lat and long, we know that the numbers are in decimal point degree. We have to tell R that it is in crs 4326 and we have to change it to 3824

## 3.6 Finding the intercept

### 3.6.1 Making it a union so that finding points within this union is faster

```{r}
merged_polygon <- st_union(tainan_filtered_villages)
```

First i make a union of all the villages

We check the classes of the objects to know what we are working with

```{r}
class(merged_polygon)
class(sf_object)
```

### 3.6.2 Intersection

```{r}
denguePointsInTainan <- st_intersection(sf_object, merged_polygon)
```

This one is to filter points found only in big polygon - it is faster than finding points in each polygon first

### 3.6.3 Finding points that fall within the different vilages, concetenating TOWNID and VILL ENG

```{r}
#| eval: false

town_dengue_intersections <- st_intersection(denguePointsInTainan, tainan_filtered_villages) %>% 
  mutate(town = paste0(as.character(TOWNID)," ",as.character(VILLENG)))

```

Comment: Finding intersection of each point mapped to each town Will need to push it into a rds as it takes very long to compute

### 3.6.4 Write to RDS

```{r}
#| eval: false
write_rds(town_dengue_intersections,"data/rds/town_dengue_intersections.rds")
```

We can see from the map here that most of the points are located in the center.

### 3.6.5 Read from RDS

```{r}
town_dengue_intersections_rds <-read_rds("data/rds/town_dengue_intersections.rds")
```

### 3.6.5 We only select the required fields which are Week number, town and transmit date

```{r}
town_dengue_intersections_rds <- select(town_dengue_intersections_rds,1,2,13)
```

## 3.7 Plots

```{r}
plot(denguePointsInTainan)
```

```{r}
tmap_mode("plot")

tm_shape(tainan_filtered_villages) +tm_polygons()+

tm_shape(denguePointsInTainan) +tm_dots()+tm_compass(type="8star",size=2)+tm_scale_bar()+tm_grid(alpha=0.2)
```

## 3.8 Joining the 2 datasets together

```{r}
df <- st_drop_geometry(town_dengue_intersections_rds)
df <- data.frame(df)
colnames(df)[3] <- "town"


tainan_filtered_villages <- tainan_filtered_villages %>%
  mutate(town = paste0(as.character(TOWNID), " ", as.character(VILLENG)))

tainan_filtered_villages2<-select(tainan_filtered_villages,4,5,8,11,12)
test2 <- left_join(df,tainan_filtered_villages2)
```

Steps: First i drop the geometric points as i already have town (comprises of TOWNID and village eng) - to seperate village level

Next, i mutate tainan_filtered_village (Map) to combine TownID and village eng too

Left join the 2 data sets and filter those columns that i want

Joining these 2 will result in date transmitted, town, and other details like geometry etc.

## 3.9 Calculating frequency

```{r}
town_frequency <- test2 %>%
  group_by(town) %>%
  summarise(frequency = n())
```

```{r}
town_frequency2 <- test2 %>%
  group_by(town,week_number) %>%
  summarise(frequency = n())
```

What this does is to calculate the number of "frequency". Frequency count will count number of cases there is a transmit date. We calculate frequency for town_frequency dataset (group only by town) and town_frequency2 (group by town and week number). Group by town and week number for spatio temporal while group by town will be for Global Measures of Spatial Autocorrelation

### 3.9.2 Joining the data

```{r}
test3 <- left_join(test2, town_frequency)
test4 <-left_join(test2,town_frequency2)
class(test3)

unique_data <- test3 %>%
  distinct(town, .keep_all = TRUE)

test3_sf <- st_as_sf(unique_data)
test4_sf <- st_as_sf(test4)
```

The end result of this join will be a dataset, each row indicating Week, town, number of transmit for that week for that town and the geometry

### 3.9.3 Visualising Regional Development Indicator

```{r}
equal <- tm_shape(test3_sf) +
  tm_fill("frequency",n=5,  style = "equal") +
  tm_borders(alpha = 0.5)+tm_compass(type="8star",size=2)+tm_scale_bar()+tm_grid(alpha=0.2) +
  tm_layout(main.title = "Town Frequency")

quantile<- tm_shape(test3_sf) +
  tm_fill("frequency",n=5,  style = "quantile") +
  tm_borders(alpha = 0.5)+tm_compass(type="8star",size=2)+tm_scale_bar()+tm_grid(alpha=0.2) +
  tm_layout(main.title = "Town Frequency")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)

```

Insight: What we can see here is that quantile frequency binning is more effective for visualizing frequency of transmit. Transmit frequency is very high in the ceter region of tainan

# 4 Global Measures of Spatial Autocorrelation

## 4.1 Computing wm_q and writing to rds

```{r}
#| eval: true
wm_q<-test3_sf %>% mutate(nb=st_contiguity(geometry), wt=st_weights(nb,style="W"),.before=1)
```

```{r}
#| eval: true
write_rds(wm_q,"data/rds/wm_qs.rds")
```

```{r}
wm_q <-read_rds("data/rds/wm_qs.rds")
```

## 4.2 Morans test

```{r}
global_moran_test(wm_q$frequency,wm_q$nb, wm_q$wt)
```

```{r}
set.seed(1234)
bperm = global_moran_perm(wm_q$frequency,
                  wm_q$nb, 
                  wm_q$wt,
                  nsim=99)
```

```{r}
summary(bperm$res[1:99])
```

```{r}
bperm
```

### 4.2.2 Visualising Monte-carlo moran's test

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=5, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

**Results of the moran's test indicate that there is strong evidence to suggest that there is significant spatial autocorrelation the data. ( p values \< 2.2e-16)**

### 4.2.3 Geary C

Arguments are the same as Moran (x, nb, wt, nsim)

```{r}
gperm = global_c_perm(wm_q$frequency,
                  wm_q$nb, 
                  wm_q$wt,
                  nsim=99)
```

```{r}
summary(gperm$res[1:99])
```

```{r}
gperm
```

```{r}
hist(gperm$res, freq=TRUE, breaks=20, xlab="Simulated Geary c")
abline(v=1, col="red") 
```

**Results of the geary c test indicate that there is strong evidence to suggest that there is significant spatial autocorrelation the data. ( p values \< 0.01) - Positive spatial autocorrelation.**

### 4.2.4 Compute Moran correlogram

```{r}
#| eval: true
MI_corr <- sp.correlogram(wm_q$nb, 
                          wm_q$frequency, 
                          order=6, 
                          method="I", 
                          style="W")
plot(MI_corr)
```

As we can see from the correlogram above, the higher the lag, the lower the Moran's I value. This means that as the lag distance increases between different towns / villages, spatial autocorrelation weakens. Values are strongest when lag are at 1 or 2.

### 4.2.5 Compute geary C correlogram

```{r}
GC_corr <- sp.correlogram(wm_q$nb, 
                          wm_q$frequency, 
                          order=6, 
                          method="C", 
                          style="W")
plot(GC_corr)
```

When lags are at 3 onward, Geary C values are close to if not 1, meaning total randomness. From here, we can see that there is positive spatial autocorrelation in neighbouring areas, only up two 2 lags (2 jumps). This could indicate that frequency in dengue are in clusters.

# 5 Local Measures of Spatial Autocorrelation (Cluster and Outlier Analysis)

## 5.1 Local moran

```{r}
localMoran = local_moran(wm_q$frequency,
                  wm_q$nb, 
                  wm_q$wt,
                  nsim=99)
```

```{r}
head(localMoran)
```

```{r}
head(unique_data[4])
```

```{r}
head(localMoran)
```

### 5.1.1 joining Local moran results and wm_q

```{r}
localMoranData <- cbind(test3_sf[2], localMoran)
```

After running local moran code, it can be binded to week number as the rows are the same at 257, no need to do any fancy join.

Code chunk below list the content of the local Moran matrix derived by using printCoefMat - followed slides (Chap10)

```{r}
#| output: false
fips <-localMoranData$town

printCoefmat(data.frame(
  localMoran, 
  row.names=localMoranData$town),
  check.names=FALSE)
```

### 5.1.2 Mapping local moran

```{r}
localMI.map <- tm_shape(localMoranData) +
  tm_fill(col = "ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)+tm_compass(type="8star",size=2)+tm_scale_bar()+tm_grid(alpha=0.2)

pvalue.map <- tm_shape(localMoranData) +
  tm_fill(col = "p_ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)+tm_compass(type="8star",size=2)+tm_scale_bar()+tm_grid(alpha=0.2)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

What does this tell us? For local moran statistics, those with values 1 and above, we can see that there is a strong spatial correlation in the data. Also, looking at the P values, it shows that our data is statistically significant to show that there is strong spatial correlation in those areas as well that are highlights in blue (from 0.001 onward)

# 6.0 Hot Spot and Cold Spot Area Analysis

If we were to use the lab example, we have to compute centroid, cut off distance, adaptive weight matrix before we can calculate Gi. However, with the use of sfdep packages, we can now do that with less steps. Source - https://sfdep.josiahparry.com/articles/understanding-emerging-hotspots

## 6.1 Calculating the local Gi\*

```{r}
getis_nb <- test3_sf |> 
  mutate(
    nb = include_self(st_contiguity(geometry)),
         wt = st_weights(nb)
    ) 
```

```{r}
gistar <- getis_nb |> 
  transmute(gi_star = local_gstar_perm(frequency, nb, wt, nsim = 199)) |> 
  tidyr::unnest(gi_star)

gistar
```

## 6.2 Mapping local Gi\*

```{r}
gistar |> 
  mutate(cluster = case_when(
    p_folded_sim > 0.05 ~ "Not Significant",
    p_folded_sim <= 0.05 & gi_star < 0 ~ "Low",
    p_folded_sim <= 0.05 & gi_star > 0 ~ "High"
  )) |> 
  ggplot(aes(fill = cluster)) +
  geom_sf(lwd = 0.2, color = "black") +
  scale_fill_manual(values = c("High" = "red",
                               "Low" = "Blue", 
                               "Not Significant" = "white")) +
  theme_void()
```
As expected, high clusters in the center region of tainan but weirdly, alot of low clusters, all on the outskirts.

# 7 Space time and space cubes

Source- https://sfdep.josiahparry.com/articles/spacetime-s3

## 7.1 Filtering data that we need

```{r}
test4_sf <- test4_sf %>%
  distinct(town,week_number, .keep_all = TRUE)

test5_sf <- test4_sf%>% select(week_number,town,frequency)
```

## 7.2 Creating a space time object

```{r}
#| output: false
spt <- as_spacetime(test5_sf, "town", "week_number")
activate(spt, "data")
```

### 7.2.1 Spacetime Contexts

```{r}
spt |> 
  activate("geometry") 

is_spacetime_cube(spt)
```

```{r}
spt_complete <- complete_spacetime_cube(spt)
```

### 7.3.2 Replacing all N.A values with 0

```{r}

spt_complete <- complete_spacetime_cube(spt)
spt_complete <- spt_complete %>%
  mutate(frequency = ifelse(is.na(frequency), 0, frequency))
```

I kept running into an error and i think its due to NA values in the frequency column. After i replaced the NA values, i didnt not run into that error anymore

## 7.4 Emerging hotspot analysis

```{r}
emerging_hotspot<- emerging_hotspot_analysis(spt_complete, "frequency", threshold = 0.05)
```

```{r}
head(emerging_hotspot)
```

## 7.5 Emerging hotspot analysis

Source - https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex05/in-class_ex05-ehsa

```{r}
#| eval: false
ehsa <- emerging_hotspot_analysis(
  x = spt_complete, 
  .var = "frequency", 
  k = 1, 
  nsim = 99
)
```

```{r}
#| eval: false
write_rds(ehsa,"data/rds/ehsa.rds")
```

```{r}
ehsa <-read_rds("data/rds/ehsa.rds")
```

### 7.5.1 Visualising the distribution of EHSA classes

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

## 7.6 Visualising EHSA

```{r}
dengue_ehsa <- test3_sf %>%
  left_join(ehsa,
            by = join_by(town == location))
```

```{r}
ehsa_sig <- dengue_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(dengue_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```
Looking at the map and the graph, majority are oscilating hotspot (highlighted in orange) and also, oscilating hotspots means there are the ones with more frequent spikes in numbers, meaning we should focus more of our attention there if we are allocating resources such as vaccines / manpower to those areas to treat dengue. 

Conclusion:
Through this assignment, i am able to successfully identify patterns in the given area in tainan and also segment them accordingly to gather insights about clusters forming. I made use of Local and global spacial autocorrelation methods to find evidence of non-randomness in dengue spreads and finally space time cube method to see how through time, hotspots/ coldspots are formed